{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 100282,
          "databundleVersionId": 12025021,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Restaurant Type Classification Using NLP"
      ],
      "metadata": {
        "id": "1_CDTvHRo6Om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author: David Lightfoot**"
      ],
      "metadata": {
        "id": "yYOmuYXytvJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy pandas scipy\n",
        "!pip install numpy==1.24.3 pandas==2.0.3 scipy==1.11.4 gensim==4.3.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "id": "tEmwU5HOmpmf",
        "outputId": "5f53f0a7-850e-4369-e78c-4461b888bcca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: scipy 1.15.3\n",
            "Uninstalling scipy-1.15.3:\n",
            "  Successfully uninstalled scipy-1.15.3\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting pandas==2.0.3\n",
            "  Downloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting scipy==1.11.4\n",
            "  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim==4.3.2\n",
            "  Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3) (2025.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (7.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.2)\n",
            "Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, pandas, gensim\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.2 numpy-1.24.3 pandas-2.0.3 scipy-1.11.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "dcb3dfc9bcd24bc1a39ddd1d42455803"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import word2vec\n",
        "import os\n",
        "import string"
      ],
      "metadata": {
        "id": "XJMXhg8vPxJI",
        "trusted": true,
        "execution": {
          "execution_failed": "2025-05-13T03:02:27.601Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import ssl\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOtcsHNEsknU",
        "outputId": "d808b2e9-30ec-43f1-f2f2-4a3a96371a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"upload train.csv file:\")\n",
        "uploaded_train = files.upload()\n",
        "print(\"upload test.csv file:\")\n",
        "uploaded_test = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "xhZP4phim6wP",
        "outputId": "f76b4bdc-f260-4f7a-eab8-f1a7fdbd30a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upload train.csv file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a19d4603-e64a-4d91-bcdf-e314b0fcb47b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a19d4603-e64a-4d91-bcdf-e314b0fcb47b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train.csv\n",
            "upload test.csv file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2a28558a-440c-4aa5-955a-4f4f3916f997\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2a28558a-440c-4aa5-955a-4f4f3916f997\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.csv to test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences):\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return word_counts, vocabulary, vocabulary_inv"
      ],
      "metadata": {
        "id": "lcZtkWvAP2IY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(inp_data, vocabulary_inv, size_features=200,\n",
        "                   mode='skipgram',\n",
        "                   min_word_count=2,\n",
        "                   context=7):\n",
        "    model_name = \"embedding\"\n",
        "    model_name = os.path.join(model_name)\n",
        "    num_workers = 15\n",
        "    downsampling = 1e-3\n",
        "    print('Training Word2Vec model...')\n",
        "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
        "    if mode == 'skipgram':\n",
        "        sg = 1\n",
        "        print('Model: skip-gram')\n",
        "    elif mode == 'cbow':\n",
        "        sg = 0\n",
        "        print('Model: CBOW')\n",
        "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
        "                                        sg=sg,\n",
        "                                        vector_size=size_features,\n",
        "                                        min_count=min_word_count,\n",
        "                                        window=context,\n",
        "                                        sample=downsampling,\n",
        "                                        epochs=20)\n",
        "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
        "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
        "    for i in range(len(vocabulary_inv)):\n",
        "        word = vocabulary_inv[i]\n",
        "        if word in embedding_model.wv:\n",
        "            embedding_weights[i] = embedding_model.wv[word]\n",
        "        else:\n",
        "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
        "                                                     embedding_model.vector_size)\n",
        "    return embedding_weights"
      ],
      "metadata": {
        "id": "BmmSg01nQDp9",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df):\n",
        "    def get_wordnet_pos(word):\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                    \"N\": wordnet.NOUN,\n",
        "                    \"V\": wordnet.VERB,\n",
        "                    \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.add('would')\n",
        "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "    preprocessed_sentences = []\n",
        "    for i, row in df.iterrows():\n",
        "        sent = row[\"text\"]\n",
        "        sent_nopuncts = sent.translate(translator)\n",
        "        words_list = sent_nopuncts.strip().split()\n",
        "        filtered_words = []\n",
        "        for word in words_list:\n",
        "            if word.lower() not in stop_words and len(word) > 1:\n",
        "                lemma = lemmatizer.lemmatize(word.lower(), get_wordnet_pos(word.lower()))\n",
        "                filtered_words.append(lemma)\n",
        "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
        "    df[\"text\"] = preprocessed_sentences\n",
        "    return df"
      ],
      "metadata": {
        "id": "n_dlSf5LQMPN",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_cuisine_keyword(text):\n",
        "    if not isinstance(text, str):\n",
        "        return np.zeros(10)\n",
        "    text = text.lower()\n",
        "    features = np.zeros(10)\n",
        "    # trackers for different cuisine types\n",
        "    asian_cuisines = []\n",
        "    non_asian_cuisines = []\n",
        "    # american (new) - index 0\n",
        "    if any(word in text for word in ['contemporary', 'farm-to-table', 'gastropub', 'artisanal']):\n",
        "        features[0] = 1\n",
        "        non_asian_cuisines.append(\"american (new)\")\n",
        "    # american (traditional) - index 1\n",
        "    if any(word in text for word in ['diner', 'homestyle', 'southern', 'bbq', 'grill']):\n",
        "        features[1] = 1\n",
        "        non_asian_cuisines.append(\"american (traditional)\")\n",
        "    # asian fusion - index 2\n",
        "    if any(word in text for word in ['fusion', 'hybrid', 'multi', 'cultural', 'blend']):\n",
        "        features[2] = 1\n",
        "    # canadian (new) - index 3\n",
        "    if any(word in text for word in ['canadian', 'poutine', 'toronto', 'montreal', 'quebec']):\n",
        "        features[3] = 1\n",
        "        non_asian_cuisines.append(\"canadian (new)\")\n",
        "    # chinese - index 4\n",
        "    if any(word in text for word in ['chinese', 'hotpot', 'wok', 'dim', 'szechuan', 'beijing']):\n",
        "        features[4] = 1\n",
        "        asian_cuisines.append(\"chinese\")\n",
        "    # italian - index 5\n",
        "    if any(word in text for word in ['italian', 'pasta', 'spaghetti', 'lasagna', 'risotto', 'carbonara']):\n",
        "        features[5] = 1\n",
        "        non_asian_cuisines.append(\"italian\")\n",
        "    # japanese - index 6\n",
        "    if any(word in text for word in ['japanese', 'sushi', 'sashimi', 'ramen', 'miso', 'udon']):\n",
        "        features[6] = 1\n",
        "        asian_cuisines.append(\"japanese\")\n",
        "    # mediterranean - index 7\n",
        "    if any(word in text for word in ['mediterranean', 'hummus', 'falafel', 'pita', 'tahini']):\n",
        "        features[7] = 1\n",
        "        non_asian_cuisines.append(\"mediterranean\")\n",
        "    # mexican - index 8\n",
        "    if any(word in text for word in ['mexican', 'taco', 'burrito', 'quesadilla', 'enchilada', 'mole']):\n",
        "        features[8] = 1\n",
        "        non_asian_cuisines.append(\"mexican\")\n",
        "    # thai - index 9\n",
        "    if any(word in text for word in ['thai', 'pad', 'tom', 'basil', 'lemongrass', 'bangkok']):\n",
        "        features[9] = 1\n",
        "        asian_cuisines.append(\"thai\")\n",
        "    # check for multiple asian cuisines\n",
        "    if len(asian_cuisines) >= 2:\n",
        "        features[2] = 1\n",
        "    # check for asian + non-asian cuisines\n",
        "    if len(asian_cuisines) >= 1 and len(non_asian_cuisines) >= 1:\n",
        "        features[2] = 1\n",
        "    return features"
      ],
      "metadata": {
        "id": "KN_Huc1EQcGe",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentiment_features(text):\n",
        "    if not isinstance(text, str):\n",
        "        return np.zeros(4)\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "    return np.array([\n",
        "        sentiment['neg'],\n",
        "        sentiment['neu'],\n",
        "        sentiment['pos'],\n",
        "        sentiment['compound']\n",
        "    ])"
      ],
      "metadata": {
        "id": "S5bt6dhHxqpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "df_train[\"text\"] = df_train[\"review\"]\n",
        "df_test[\"text\"] = df_test[\"review\"]\n",
        "df_train = preprocess_df(df_train)\n",
        "df_test = preprocess_df(df_test)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_train[\"encoded_label\"] = label_encoder.fit_transform(df_train[\"label\"])\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(f\"Label mapping: {label_mapping}\")\n",
        "\n",
        "tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
        "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
        "inp_data = [[vocabulary[word] for word in text] for text in tagged_data]\n",
        "embedding_weights = get_embeddings(inp_data, vocabulary_inv)\n",
        "\n",
        "tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
        "tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test[\"text\"])]\n",
        "\n",
        "cuisine_terms = set()\n",
        "for cuisine_list in [\n",
        "    ['contemporary', 'farm-to-table', 'gastropub', 'artisanal'],  # american (new)\n",
        "    ['diner', 'homestyle', 'southern', 'bbq', 'grill'],  # american (traditional)\n",
        "    ['fusion', 'hybrid', 'multi', 'cultural', 'blend'],  # asian fusion\n",
        "    ['canadian', 'poutine', 'toronto', 'montreal', 'quebec'],  # canadian (new)\n",
        "    ['chinese', 'hotpot', 'wok', 'dim', 'szechuan', 'beijing'],  # chinese\n",
        "    ['italian', 'pasta', 'spaghetti', 'lasagna', 'risotto', 'carbonara'],  # italian\n",
        "    ['japanese', 'sushi', 'sashimi', 'ramen', 'miso', 'udon'],  # japanese\n",
        "    ['mediterranean', 'hummus', 'falafel', 'pita', 'tahini'],  # mediterranean\n",
        "    ['mexican', 'taco', 'burrito', 'quesadilla', 'enchilada', 'mole'],  # mexican\n",
        "    ['thai', 'pad', 'tom', 'basil', 'lemongrass', 'bangkok']  # thai\n",
        "]:\n",
        "    cuisine_terms.update(cuisine_list)\n",
        "cuisine_weight = 4.0 # higher weight for cuisine terms to improve classification accuracy\n",
        "print(f\"Using fixed cuisine weight: {cuisine_weight}\")\n",
        "\n",
        "train_vec = []\n",
        "for doc in tagged_train_data:\n",
        "    vec = 0\n",
        "    total_weight = 0\n",
        "    for w in doc:\n",
        "        weight = cuisine_weight if w.lower() in cuisine_terms else 1.0\n",
        "        vec += embedding_weights[vocabulary[w]] * weight\n",
        "        total_weight += weight\n",
        "    vec = vec / total_weight if total_weight > 0 else vec\n",
        "    train_vec.append(vec)\n",
        "\n",
        "test_vec = []\n",
        "for doc in tagged_test_data:\n",
        "    vec = 0\n",
        "    total_weight = 0\n",
        "    for w in doc:\n",
        "        try:\n",
        "            weight = cuisine_weight if w.lower() in cuisine_terms else 1.0\n",
        "            vec += embedding_weights[vocabulary[w]] * weight\n",
        "            total_weight += weight\n",
        "        except KeyError:\n",
        "            continue\n",
        "    vec = vec / total_weight if total_weight > 0 else vec\n",
        "    test_vec.append(vec)\n",
        "\n",
        "train_cuisine_features = np.array([extract_cuisine_keyword(text) for text in df_train[\"text\"]])\n",
        "test_cuisine_features = np.array([extract_cuisine_keyword(text) for text in df_test[\"text\"]])\n",
        "train_sentiment_features = np.array([extract_sentiment_features(text) for text in df_train[\"review\"]])\n",
        "test_sentiment_features = np.array([extract_sentiment_features(text) for text in df_test[\"review\"]])\n",
        "\n",
        "train_features = np.hstack([\n",
        "    np.array(train_vec),\n",
        "    train_cuisine_features,\n",
        "    train_sentiment_features\n",
        "])\n",
        "\n",
        "test_features = np.hstack([\n",
        "    np.array(test_vec),\n",
        "    test_cuisine_features,\n",
        "    test_sentiment_features\n",
        "])\n",
        "\n",
        "np.random.seed(42)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(df_train[\"encoded_label\"]), y=df_train[\"encoded_label\"])\n",
        "sample_weights = np.array([class_weights[i] for i in df_train[\"encoded_label\"]])\n",
        "\n",
        "clf = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.08,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(train_features, df_train[\"encoded_label\"], sample_weight=sample_weights)\n",
        "\n",
        "numeric_preds = clf.predict(test_features)\n",
        "preds = label_encoder.inverse_transform(numeric_preds)\n",
        "\n",
        "dic = {\"Id\": [], \"Predicted\": []}\n",
        "for i, pred in enumerate(preds):\n",
        "    dic[\"Id\"].append(i)\n",
        "    dic[\"Predicted\"].append(pred)\n",
        "\n",
        "dic_df = pd.DataFrame.from_dict(dic)\n",
        "dic_df.to_csv(\"predicted.csv\", index=False)\n",
        "\n",
        "print(f\"done\")"
      ],
      "metadata": {
        "id": "6yQxjmMgRJrP",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4ba30f-b76c-4019-9e2a-8a65e40921d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label mapping: {'american (new)': 0, 'american (traditional)': 1, 'asian fusion': 2, 'canadian (new)': 3, 'chinese': 4, 'italian': 5, 'japanese': 6, 'mediterranean': 7, 'mexican': 8, 'thai': 9}\n",
            "Training Word2Vec model...\n",
            "Model: skip-gram\n",
            "Saving Word2Vec model embedding\n",
            "Using fixed cuisine weight: 4.0\n",
            "done\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('predicted.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4kFrjhmcngvl",
        "outputId": "c95a1c49-94ea-42d0-e5d7-04a6fa6ae340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a3849517-4e45-4395-ad88-6b22b4d5172e\", \"predicted.csv\", 175262)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}